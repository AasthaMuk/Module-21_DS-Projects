{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT-1 : TWITTER SENTIMENT ANALYSIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following project is about analyzing the sentiments of tweets on social networking website‘Twitter’. The dataset for this project  is scraped from Twitter. It contains 1,600,000 tweetsextracted using Twitter API. It is a labeled dataset with tweets annotated with the sentiment (0 =negative,2= neutral,4 = positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains the following 6 fields:\n",
    " 1. target: the polarity of the tweet (0= negative,2= neutral,4= positive)\n",
    " 2. ids: The id of the tweet .\n",
    " 3. date: The date of the tweet (Sat May 16 23:58:44 UTC2009)\n",
    " 4. flag: The query. If there is no query, then this value is NO_QUERY.\n",
    " 5. user: The user that tweeted\n",
    " 6. text: The text of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a classification model that correctly predicts the polarity of the tweets provided in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing normal libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing classifier and ML-related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r'C:\\Users\\aasth\\OneDrive\\Desktop\\Guvi_Projects_Module_21\\Project1-Twitter Sentiment Analysis\\dataset\\twitter_new.csv',\n",
    "#                  encoding='ISO-8859-1',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['target','ids','date','flag','user','text']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x : ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(x):\n",
    "    x = x.split()\n",
    "    for i in range(0,len(x)):\n",
    "        if \"@\" in x[i]:\n",
    "            x[i]= x[i].replace(\"@\",\"\")\n",
    "        if \"#\" in x[i]:\n",
    "            x[i]=x[i].replace(\"#\",\"\")\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']= df['text'].apply(lambda x: remove_special_char(x))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The isdigit() method returns True if all characters in a string are digits. If not, it returns False\n",
    "def remove_digits(x):\n",
    "    x = x.split()\n",
    "    for i in range(0,len(x)):\n",
    "        if x[i].isdigit():\n",
    "            x[i]= \"\"\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_digits(x))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting each word in tweets to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x:x.lower())\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(x):\n",
    "    x = x.split()\n",
    "    for i in range(0,len(x)):\n",
    "        x[i] = contractions.fix(x[i])\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "print(contractions.fix(\"you've\"))\n",
    "print(contractions.fix(\"he's\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x:expand_contractions(x))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of emails from the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1['emails'] = df1['text'].apply(lambda x: re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)', x))\n",
    "df1['email_count'] = df1['emails'].apply(lambda x: len(x))\n",
    "df1[df1['email_count'] > 0]\n",
    "\n",
    "# We can see there are no email_ids present in tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['special_char'] = df1['text'].apply(lambda x: re.findall(r'[^\\w]+', x))\n",
    "df1['special_char_count'] = df1['special_char'].apply(lambda x: len(x))\n",
    "df1['special_char_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x : re.sub(r'[^\\w]+',\" \",x))\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['url'] = df1['text'].apply(lambda x: re.findall(pattern, x))\n",
    "df1['url_count'] = df1['url'].apply(lambda x: len(x))\n",
    "df1['url_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x : re.sub(pattern,\" \",x))\n",
    "df['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
